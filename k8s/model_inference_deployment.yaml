apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-inference
  labels:
    app: model-inference
spec:
  replicas: 1
  revisionHistoryLimit: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: model-inference
  template:
    metadata:
      labels:
        app: model-inference
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: api
          image: model-inference:latest
          imagePullPolicy: Never
          workingDir: /app
          command: ["python"]
          args: ["/app/src/inference.py"]
          ports:
            - name: http
              containerPort: 5002
              protocol: TCP
          env:
            - name: HOST
              value: "0.0.0.0"
            - name: PORT
              value: "5002"
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: OMP_NUM_THREADS
              value: "1"
            - name: MKL_NUM_THREADS
              value: "1"
            - name: NUMBA_NUM_THREADS
              value: "1"
            - name: OPENBLAS_NUM_THREADS
              value: "1"
          resources:
            requests:
              cpu: "250m"
              memory: "512Mi"
            limits:
              cpu: "1"
              memory: "1.5Gi"
          readinessProbe:
            tcpSocket:
              port: 5002
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 6
          livenessProbe:
            tcpSocket:
              port: 5002
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
          startupProbe:
            tcpSocket:
              port: 5002
            periodSeconds: 5
            failureThreshold: 60
